{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:06:48.924782Z",
     "start_time": "2024-02-22T12:06:48.342145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       textID                                               text sentiment  \\\n0  b8f4e560fa   but yeah i like purple maybe thats why!! ;)  ...  positive   \n1  f81a1511b2  _Henrie haha i WISH i coudl meet you.. you sho...  positive   \n2  3e9e3f0d69          nah, you`re just altered forever   Enjoy.  positive   \n3  a068b95bd9   Can you ask Ryan why he stopped following me ...  negative   \n4  e6da2d1835  New issue of  in the office...desperately want...  negative   \n\n         message_date account_creation_date  \\\n0   2023-10-2 7:41:52      2015-4-13 0:1:16   \n1   2022-11-17 2:1:57     2013-7-1 18:58:16   \n2    2022-6-1 6:53:18      2013-8-8 2:21:12   \n3   2022-1-4 11:38:57   2015-10-16 14:51:19   \n4  2022-5-11 21:56:27    2013-12-15 19:8:40   \n\n                             previous_messages_dates  \\\n0  [2019-2-15 16:12:42, 2020-6-2 16:49:53, 2021-4...   \n1  [2021-7-5 21:53:48, 2020-12-12 9:7:50, 2020-9-...   \n2  [2021-7-11 9:59:36, 2020-2-18 6:38:45, 2021-5-...   \n3  [2021-8-16 4:55:32, 2021-6-1 10:13:37, 2021-12...   \n4  [2021-9-11 10:55:6, 2021-5-6 12:1:54, 2021-1-1...   \n\n                                date_of_new_follower  \\\n0  [2016-1-13 7:12:26, 2016-5-17 13:40:38, 2016-1...   \n1  [2016-10-4 7:49:46, 2016-6-10 7:47:39, 2016-4-...   \n2  [2018-7-13 17:0:3, 2016-2-13 17:8:12, 2016-2-1...   \n3                                  [2018-7-1 5:9:39]   \n4           [2018-11-2 11:16:12, 2018-10-18 16:8:34]   \n\n                                  date_of_new_follow  \\\n0  [2016-7-15 0:3:50, 2016-4-10 22:31:8, 2016-10-...   \n1  [2016-7-5 20:23:57, 2016-5-3 15:9:51, 2018-11-...   \n2  [2016-7-9 22:15:10, 2018-4-13 20:37:55, 2016-1...   \n3  [2018-10-18 7:23:20, 2018-4-10 12:17:6, 2016-6...   \n4  [2018-4-18 20:56:34, 2018-7-1 14:56:33, 2016-4...   \n\n                              email gender email_verified blue_tick  \\\n0  ClareRosindill3265@messenger.gov      F            NaN     False   \n1   ErenaAntonchik231@messenger.gov      M           True      True   \n2          GodardCowlas6687@api.gov      F           True      True   \n3     RalfStolberger6467@python.gov      M          False     False   \n4         MadelLeaton248@bbc.gov.il      M          False     False   \n\n  embedded_content  platform  \n0              mp4  facebook  \n1            False    tiktok  \n2            False    tiktok  \n3              mp4    tiktok  \n4             None      None  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>sentiment</th>\n      <th>message_date</th>\n      <th>account_creation_date</th>\n      <th>previous_messages_dates</th>\n      <th>date_of_new_follower</th>\n      <th>date_of_new_follow</th>\n      <th>email</th>\n      <th>gender</th>\n      <th>email_verified</th>\n      <th>blue_tick</th>\n      <th>embedded_content</th>\n      <th>platform</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b8f4e560fa</td>\n      <td>but yeah i like purple maybe thats why!! ;)  ...</td>\n      <td>positive</td>\n      <td>2023-10-2 7:41:52</td>\n      <td>2015-4-13 0:1:16</td>\n      <td>[2019-2-15 16:12:42, 2020-6-2 16:49:53, 2021-4...</td>\n      <td>[2016-1-13 7:12:26, 2016-5-17 13:40:38, 2016-1...</td>\n      <td>[2016-7-15 0:3:50, 2016-4-10 22:31:8, 2016-10-...</td>\n      <td>ClareRosindill3265@messenger.gov</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>mp4</td>\n      <td>facebook</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f81a1511b2</td>\n      <td>_Henrie haha i WISH i coudl meet you.. you sho...</td>\n      <td>positive</td>\n      <td>2022-11-17 2:1:57</td>\n      <td>2013-7-1 18:58:16</td>\n      <td>[2021-7-5 21:53:48, 2020-12-12 9:7:50, 2020-9-...</td>\n      <td>[2016-10-4 7:49:46, 2016-6-10 7:47:39, 2016-4-...</td>\n      <td>[2016-7-5 20:23:57, 2016-5-3 15:9:51, 2018-11-...</td>\n      <td>ErenaAntonchik231@messenger.gov</td>\n      <td>M</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>tiktok</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3e9e3f0d69</td>\n      <td>nah, you`re just altered forever   Enjoy.</td>\n      <td>positive</td>\n      <td>2022-6-1 6:53:18</td>\n      <td>2013-8-8 2:21:12</td>\n      <td>[2021-7-11 9:59:36, 2020-2-18 6:38:45, 2021-5-...</td>\n      <td>[2018-7-13 17:0:3, 2016-2-13 17:8:12, 2016-2-1...</td>\n      <td>[2016-7-9 22:15:10, 2018-4-13 20:37:55, 2016-1...</td>\n      <td>GodardCowlas6687@api.gov</td>\n      <td>F</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>tiktok</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a068b95bd9</td>\n      <td>Can you ask Ryan why he stopped following me ...</td>\n      <td>negative</td>\n      <td>2022-1-4 11:38:57</td>\n      <td>2015-10-16 14:51:19</td>\n      <td>[2021-8-16 4:55:32, 2021-6-1 10:13:37, 2021-12...</td>\n      <td>[2018-7-1 5:9:39]</td>\n      <td>[2018-10-18 7:23:20, 2018-4-10 12:17:6, 2016-6...</td>\n      <td>RalfStolberger6467@python.gov</td>\n      <td>M</td>\n      <td>False</td>\n      <td>False</td>\n      <td>mp4</td>\n      <td>tiktok</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e6da2d1835</td>\n      <td>New issue of  in the office...desperately want...</td>\n      <td>negative</td>\n      <td>2022-5-11 21:56:27</td>\n      <td>2013-12-15 19:8:40</td>\n      <td>[2021-9-11 10:55:6, 2021-5-6 12:1:54, 2021-1-1...</td>\n      <td>[2018-11-2 11:16:12, 2018-10-18 16:8:34]</td>\n      <td>[2018-4-18 20:56:34, 2018-7-1 14:56:33, 2016-4...</td>\n      <td>MadelLeaton248@bbc.gov.il</td>\n      <td>M</td>\n      <td>False</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.stats import chi2_contingency\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_pickle(r'/Users/maya/Documents/Information Systems/שנה ג׳/למידת מכונה/פרוייקט קורס/XY_train.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7be614184960251f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Calculate missing values before deletion\n",
    "    missing_values_before = df.isnull().sum()\n",
    "\n",
    "    # Drop rows with missing values exceeding a threshold\n",
    "    df = df.dropna(thresh=df.shape[1] - 2)\n",
    "\n",
    "    missing_values_after = df.isnull().sum()\n",
    "    missing_values_table = pd.DataFrame({\n",
    "        'Before': missing_values_before,\n",
    "        'After': missing_values_after,\n",
    "        'Difference': missing_values_before - missing_values_after\n",
    "    })\n",
    "    print(\"Missing Values Before and After Deletion:\")\n",
    "    print(missing_values_table)\n",
    "\n",
    "    # Fill missing values for 'email' with 'unknown'\n",
    "    df['email'] = df['email'].fillna('unknown')\n",
    "\n",
    "    # Fill missing values for 'embedded_content' and 'platform' based on their probability distributions\n",
    "    embedded_content_prob = df['embedded_content'].value_counts(normalize=True)\n",
    "    platform_prob = df['platform'].value_counts(normalize=True)\n",
    "\n",
    "    def impute_missing_values(row, prob_dist):\n",
    "        if pd.isnull(row):\n",
    "            return np.random.choice(prob_dist.index, p=prob_dist.values)\n",
    "        else:\n",
    "            return row\n",
    "\n",
    "    df['embedded_content'] = df['embedded_content'].apply(lambda x: impute_missing_values(x, embedded_content_prob))\n",
    "    df['platform'] = df['platform'].apply(lambda x: impute_missing_values(x, platform_prob))\n",
    "\n",
    "    # Fill missing values for 'email_verified' and 'blue_tick'\n",
    "    df['email_verified'].fillna(df['blue_tick'], inplace=True)\n",
    "    df['blue_tick'].fillna(df['email_verified'], inplace=True)\n",
    "\n",
    "    # Fill missing values for 'gender' randomly\n",
    "    df['gender'].replace('None', np.nan, inplace=True)\n",
    "    gender_counts = df['gender'].value_counts()\n",
    "    df['gender'].fillna(pd.Series(np.random.choice(gender_counts.index, size=len(df.index),\n",
    "                                                   p=(gender_counts / gender_counts.sum()))), inplace=True)\n",
    "\n",
    "    # Delete the remaining rows with missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert 'message_date' to categorical and create 'message_time_category' column\n",
    "    df['message_date'] = pd.to_datetime(df['message_date'])\n",
    "    df['hour'] = df['message_date'].dt.hour\n",
    "    morning_interval = range(6, 12)  # 6:00 AM to 11:59 AM\n",
    "    noon_interval = range(12, 18)    # 12:00 PM to 5:59 PM\n",
    "    evening_interval = range(18, 24) # 6:00 PM to 11:59 PM\n",
    "\n",
    "    def categorize_hour(hour):\n",
    "        if hour in morning_interval:\n",
    "            return 'Morning'\n",
    "        elif hour in noon_interval:\n",
    "            return 'Noon'\n",
    "        elif hour in evening_interval:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "\n",
    "    df['message_time_category'] = df['hour'].apply(categorize_hour)\n",
    "    df.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "    # Lower case\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(stemmer.stem(word) for word in x.split()))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "\n",
    "    # Remove numbers\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "    # Remove words with 1 letter\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\b\\w{1}\\b', '', x))\n",
    "\n",
    "    # Remove top 0.05% of most common or not common words\n",
    "    h_pct = 0.05\n",
    "    l_pct = 0.05\n",
    "\n",
    "    # Remove the top $h_pct of the most frequent words\n",
    "    high_freq = pd.Series(' '.join(df['text']).split()).value_counts()[:int(pd.Series(' '.join(df['text']).split()).count() * h_pct / 100)]\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(x for x in x.split() if x not in high_freq))\n",
    "\n",
    "    # Remove the top $l_pct of the least frequent words\n",
    "    low_freq = pd.Series(' '.join(df['text']).split()).value_counts()[:-int(pd.Series(' '.join(df['text']).split()).count() * l_pct / 100):-1]\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(x for x in x.split() if x not in low_freq))\n",
    "\n",
    "    # Remove double spaces\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6447e1d7efcfde6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "757968e1e6b38f1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_features(df):\n",
    "    # 1. Create a new column based on the length of messages\n",
    "    df['message_length'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "    # 2. Create the number of messages sent by the user\n",
    "    df['num_messages_sent'] = df['previous_messages_dates'].apply(len)\n",
    "\n",
    "    # 3. Create the number of followers and following\n",
    "    df['follower_count'] = df['date_of_new_follower'].apply(lambda x: len(x))\n",
    "    df['following_count'] = df['date_of_new_follow'].apply(lambda x: len(x))\n",
    "\n",
    "    # 4. Append new columns created to the df\n",
    "    new_columns_df = df[['follower_count', 'following_count']]\n",
    "\n",
    "    # 5. N-GRAM\n",
    "    X = df['clean_text']\n",
    "    ngram_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=100)\n",
    "    X_ngrams = ngram_vectorizer.fit_transform(X).toarray()\n",
    "    df_output = pd.DataFrame(data=X_ngrams, columns=ngram_vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 6. Extract email domain endings\n",
    "    def extract_email_domain_ending(email):\n",
    "        if pd.isnull(email):\n",
    "            return 'Missing'\n",
    "        match = re.search(r'\\.(\\w+)$', email)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    df['email_domain_ending'] = df['email'].apply(extract_email_domain_ending)\n",
    "    email_domain_ending_counts = df['email_domain_ending'].value_counts()\n",
    "\n",
    "    # 7. Create seniority in years\n",
    "    df['account_creation_date'] = pd.to_datetime(df['account_creation_date'])\n",
    "    current_date = datetime.now()\n",
    "    df['seniority'] = (current_date - df['account_creation_date']).dt.days / 365.25\n",
    "\n",
    "    # 8. Create the average time difference between messages\n",
    "    def calculate_average_time_difference(message_dates_array):\n",
    "        message_dates = [datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S') for date_str in message_dates_array]\n",
    "        time_diffs = []\n",
    "        for i in range(1, len(message_dates)):\n",
    "            time_diff = (message_dates[i] - message_dates[i - 1]).total_seconds()\n",
    "            if time_diff < 0:\n",
    "                time_diff = (message_dates[i - 1] - message_dates[i]).total_seconds()\n",
    "            time_diffs.append(time_diff)\n",
    "        if len(time_diffs) > 0:\n",
    "            average_time_difference = sum(time_diffs) / len(time_diffs)\n",
    "            average_time_difference = int(average_time_difference)\n",
    "            return average_time_difference\n",
    "        else:\n",
    "            return None\n",
    "    df['average_time_difference'] = df['previous_messages_dates'].apply(calculate_average_time_difference)\n",
    "\n",
    "    return df, new_columns_df, df_output\n",
    "# \n",
    "# # Assuming 'df' is your DataFrame\n",
    "# df, new_columns_df, df_output = extract_features(df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7949465137706e8f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Representation Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16fc40feb27b9fa9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def feature_representation(df, X_ngrams, ngram_vectorizer):\n",
    "    # Normalize the values by dividing each column by its maximum value\n",
    "    columns_to_normalize = ['message_length', 'num_messages_sent', 'follower_count', 'following_count', 'seniority']\n",
    "    for column in columns_to_normalize:\n",
    "        max_value = df[column].max()\n",
    "        df[f'normalized_{column}'] = df[column] / max_value\n",
    "\n",
    "    # One-hot coding\n",
    "    email_domain_ending_onehot = pd.get_dummies(df['email_domain_ending'], prefix='email_ending')\n",
    "    embedded_content_onehot = pd.get_dummies(df['embedded_content'], prefix='embedded_content')\n",
    "    platform_onehot = pd.get_dummies(df['platform'], prefix='platform')\n",
    "    message_time_category_onehot = pd.get_dummies(df['message_time_category'], prefix='message_time')\n",
    "\n",
    "    df = pd.concat([df, email_domain_ending_onehot, embedded_content_onehot, platform_onehot, message_time_category_onehot], axis=1)\n",
    "    df.drop(['email_domain_ending', 'embedded_content', 'platform', 'message_time_category'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert one hot coding to binary values\n",
    "    bool_to_binary = {True: 1, False: 0}\n",
    "    df['email_verified'] = df['email_verified'].map(bool_to_binary)\n",
    "    df['blue_tick'] = df['blue_tick'].map(bool_to_binary)\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == bool:\n",
    "            df[column] = df[column].astype(int) \n",
    "\n",
    "    gender_to_binary = {'F': 1, 'M': 0}\n",
    "    df['gender'] = df['gender'].map(gender_to_binary)\n",
    "\n",
    "    # Normalization for the n-gram features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_ngrams_normalized = scaler.fit_transform(X_ngrams)\n",
    "    df_normalized = pd.DataFrame(X_ngrams_normalized, columns=ngram_vectorizer.get_feature_names_out())\n",
    "    df = pd.concat([df.reset_index(drop=True), df_normalized], axis=1)\n",
    "\n",
    "    # Arrange data again \n",
    "    sentiment_mapping = {'positive': 1, 'negative': -1}\n",
    "    df['sentiment'] = df['sentiment'].map(sentiment_mapping)\n",
    "    df = df.drop(columns=['text', 'previous_messages_dates', 'message_date',\n",
    "                          'email', 'date_of_new_follower', 'date_of_new_follow',\n",
    "                          'account_creation_date', 'message_length', 'num_messages_sent',\n",
    "                          'follower_count', 'following_count', \n",
    "                          'seniority', 'clean_text', 'average_time_difference'])\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1bd77d929d852c5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Train indices: [    0     1     2 ... 12268 12269 12271]\n",
      "Test indices: [   14    19    31 ... 12229 12236 12270]\n",
      "\n",
      "Fold 2:\n",
      "Train indices: [    1     2     4 ... 12269 12270 12271]\n",
      "Test indices: [    0     3     8 ... 12235 12254 12263]\n",
      "\n",
      "Fold 3:\n",
      "Train indices: [    0     1     2 ... 12269 12270 12271]\n",
      "Test indices: [   10    23    29 ... 12239 12256 12265]\n",
      "\n",
      "Fold 4:\n",
      "Train indices: [    0     1     2 ... 12268 12269 12270]\n",
      "Test indices: [   12    20    30 ... 12224 12241 12271]\n",
      "\n",
      "Fold 5:\n",
      "Train indices: [    0     1     2 ... 12269 12270 12271]\n",
      "Test indices: [   26    27    51 ... 12215 12251 12260]\n",
      "\n",
      "Fold 6:\n",
      "Train indices: [    0     1     2 ... 12269 12270 12271]\n",
      "Test indices: [   15    28    34 ... 12240 12261 12264]\n",
      "\n",
      "Fold 7:\n",
      "Train indices: [    0     1     3 ... 12269 12270 12271]\n",
      "Test indices: [    2    18    43 ... 12259 12266 12268]\n",
      "\n",
      "Fold 8:\n",
      "Train indices: [    0     1     2 ... 12269 12270 12271]\n",
      "Test indices: [    6     7    22 ... 12244 12249 12262]\n",
      "\n",
      "Fold 9:\n",
      "Train indices: [    0     2     3 ... 12268 12270 12271]\n",
      "Test indices: [    1    11    13 ... 12255 12267 12269]\n",
      "\n",
      "Fold 10:\n",
      "Train indices: [    0     1     2 ... 12269 12270 12271]\n",
      "Test indices: [    4     5     9 ... 12248 12253 12258]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "x = df.drop(columns=['sentiment', 'textID']) \n",
    "y = df['sentiment']\n",
    "\n",
    "# Step 2: Initialize your model (e.g., Logistic Regression)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Step 3: Perform k-fold cross-validation using the processed dataset\n",
    "k = 10 # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(df):\n",
    "    X_train, X_test = df.iloc[train_index], df.iloc[test_index]\n",
    "    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Visualize the dataset splitting for each fold\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(df):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices:\", test_index)\n",
    "    print()\n",
    "    fold += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:21:27.882715Z",
     "start_time": "2024-02-22T12:21:27.837933Z"
    }
   },
   "id": "d7629f629c7ba48f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c09a258bd99afe02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
